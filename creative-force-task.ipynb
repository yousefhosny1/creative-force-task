{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://cf-data-engineer.glitch.me\"\n",
    "\n",
    "customers_df    = pd.DataFrame.from_records(requests.get(BASE_URL + \"/customer\").json())\n",
    "lineitems_df    = pd.DataFrame.from_records(requests.get(BASE_URL + \"/lineitem\").json())\n",
    "nations_df      = pd.DataFrame.from_records(requests.get(BASE_URL + \"/nation\").json())\n",
    "orders_df       = pd.DataFrame.from_records(requests.get(BASE_URL + \"/orders\").json())\n",
    "parts_df        = pd.DataFrame.from_records(requests.get(BASE_URL + \"/part\").json())\n",
    "partsupp_df     = pd.DataFrame.from_records(requests.get(BASE_URL + \"/partsupp\").json())\n",
    "regions_df      = pd.DataFrame.from_records(requests.get(BASE_URL + \"/region\").json())\n",
    "suppliers_df    = pd.DataFrame.from_records(requests.get(BASE_URL + \"/supplier\").json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Schema design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Star Schema** is supposed to satisfy analysis goals, thus, before I design a star schema I need to identify what are the analysis goals.\n",
    "\n",
    "Since analysis goals are not given (other than those in **Step 5 - Quiz Time**), I will make few assumptions before designing the star schema, first of all,lets assume that our main measure will be ```revenue``` and ```quantity``` and we want to analyze these measures with respect to ```customers```, ```suppliers```, ```time```, ```parts```, and ```order details```. \n",
    "\n",
    "Looking at the raw data that we've extracted from the API, we will find that our ```customer``` table & ```supplier``` table have **foreign key** that's referring to the ```nations``` table, since our goal is to design a star schema, not a snowflake schema, I will have to denormalize the customer and supplier tables. This means that I will include the details of the customer's nation & region in a single customer table and I will include the details of the supplier's nation & region in a single supplier table.\n",
    "\n",
    "Now we have 5 dimension tables, and 1 fact table, each observation/row in the fact table ```lineitem_orders_fact``` will contain foreign keys that will refer to the 5 dimension ```customers, suppliers, date, parts, order details``` and this will enable us to conduct roll up and drill down analysis with respect to our dimensions, for example if we want to analyze revenue with respect to customer, we can achieve this, if we want to drill down and analyze with respect to customers at a specific region, we can do that and if we want to drill down further and analyze with respect to customers at a specific region and a specific nation, we can easily do that.\n",
    "\n",
    "Finally, I ommitted few attributes that don't make sense for our analysis, for example, **the shipping_date**, **receipt_date**, **return_flag** in the ```lineitems``` data needs to be dropped since this will mean that we will have to wait for the shipment to arrive to our customer and record it's date before populating our star schema, which means that alot of the orders that were already placed will be delayed before being put in the data warehouse for analysis. Also the **comments** attribute in all tables don't make sense for our drill-down / roll-up analysis, thus they wont be considered. The **linestatus** in the ```lineitem``` data also doesn't make sense for our analysis as well, thus it will not be considered.\n",
    "\n",
    "After all of the above considerations, the star schema is as following\n",
    "\n",
    "[![Untitled-1.png](https://i.postimg.cc/sXHNs4hR/Untitled-1.png)](https://postimg.cc/ph8ZB8C0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create the star schema tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create date dimension table\n",
    "def create_date_dim_table():\n",
    "    def convert_date(x):\n",
    "        splitted_x = x.split('-')\n",
    "        d_datekey = x\n",
    "        d_year = splitted_x[0]\n",
    "        d_month = splitted_x[1]\n",
    "        d_day = splitted_x[2]\n",
    "\n",
    "        return pd.Series([d_datekey, d_year, d_month, d_day])\n",
    "\n",
    "    date_df = pd.DataFrame(columns=['d_datekey', 'd_year', 'd_month', 'd_day'])\n",
    "\n",
    "    date_df[[\"d_datekey\", \"d_year\", \"d_month\", \"d_day\"]\n",
    "            ] = orders_df[\"o_orderdate\"].apply(convert_date).drop_duplicates()\n",
    "\n",
    "    return date_df\n",
    "\n",
    "\n",
    "# creates customer dimension table\n",
    "def create_customer_dim_table():\n",
    "\n",
    "    customer_dim_table = customers_df.join(nations_df, on=\"c_nationkey\").join(regions_df, on=\"n_regionkey\"). \\\n",
    "        drop(columns=[\"n_nationkey\", \"c_nationkey\", \"n_regionkey\",\n",
    "             \"r_regionkey\", \"n_comment\",  \"r_comment\", \"c_comment\"])\n",
    "\n",
    "    customer_dim_table.rename(\n",
    "        columns={\"n_name\": \"c_nation\", \"r_name\": \"c_region\"}, inplace=True)\n",
    "\n",
    "    # classifying customers using quantiles based on their account balance\n",
    "    customer_dim_table[\"c_accbal_class\"] = pd.qcut(customers_df[\"c_acctbal\"], q=[\n",
    "                                                   \"0\", \"0.25\", \"0.75\", \"1\"], labels=[\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "    return customer_dim_table\n",
    "\n",
    "\n",
    "# creates supplier dimension table\n",
    "def create_supp_dim_table():\n",
    "    supplier_dim_table = suppliers_df. \\\n",
    "        join(nations_df, on=\"s_nationkey\"). \\\n",
    "        join(regions_df, on=\"n_regionkey\"). \\\n",
    "        drop(columns=[\"n_nationkey\", \"s_nationkey\", \"n_regionkey\",\n",
    "             \"r_regionkey\", \"n_comment\",  \"r_comment\", \"s_comment\"])\n",
    "\n",
    "    supplier_dim_table.rename(\n",
    "        columns={\"n_name\": \"s_nation\", \"r_name\": \"s_region\"}, inplace=True)\n",
    "\n",
    "    return supplier_dim_table\n",
    "\n",
    "\n",
    "# creates part dimension table\n",
    "def create_part_dim_table():\n",
    "    return parts_df.drop(columns=[\"p_comment\"])\n",
    "\n",
    "\n",
    "# creates order dimension table\n",
    "def create_orders_dim_table():\n",
    "    return orders_df.drop(columns=[\"o_comment\"])\n",
    "\n",
    "\n",
    "# creates lineitem orders fact table\n",
    "def create_lineorders_fact_table():\n",
    "    lineorders_fact_table = lineitems_df. \\\n",
    "        join(orders_df.set_index(\"o_orderkey\"), on=\"l_orderkey\"). \\\n",
    "        drop(columns=[\n",
    "            \"l_comment\",\n",
    "            \"l_shipdate\",\n",
    "            \"l_commitdate\",\n",
    "            \"l_receiptdate\",\n",
    "            \"l_returnflag\",\n",
    "            \"o_comment\",\n",
    "            \"o_orderpriority\",\n",
    "            \"o_shippriority\",\n",
    "            \"o_clerk\",\n",
    "            \"o_orderstatus\",\n",
    "            \"l_linestatus\",\n",
    "            \"o_totalprice\"])\n",
    "\n",
    "    lineorders_fact_table.rename(\n",
    "        columns={\"o_custkey\": \"l_custkey\", \"o_orderdate\": \"l_datekey\"}, inplace=True)\n",
    "\n",
    "    # computing the revenue\n",
    "    lineorders_fact_table[\"l_revenue\"] = lineorders_fact_table[\"l_extendedprice\"] * (\n",
    "        1 - lineorders_fact_table[\"l_discount\"])\n",
    "\n",
    "    return lineorders_fact_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** In order to run the rest of the notebook, you'll have to ```docker-compose up``` the ```docker-compose.yaml``` to spin up the postgres database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating the Star Schema (In POSTGRES Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# configuring the connection to postgres\n",
    "engine = create_engine('postgresql://postgres:postgres@localhost:5432/datawarehouse')\n",
    "\n",
    "# creating the star schema tables\n",
    "lineitem_orders_fact = create_lineorders_fact_table()\n",
    "dim_customer = create_customer_dim_table()\n",
    "dim_supplier = create_supp_dim_table()\n",
    "dim_order = create_orders_dim_table()\n",
    "dim_part = create_part_dim_table()\n",
    "dim_date = create_date_dim_table()\n",
    "\n",
    "# loading the data in postgres\n",
    "lineitem_orders_fact.to_sql(\"lineitem_orders_fact\", con = engine, index = False)\n",
    "dim_customer.to_sql(\"dim_customer\", con = engine, index = False)\n",
    "dim_supplier.to_sql(\"dim_supplier\", con = engine, index = False)\n",
    "dim_order.to_sql(\"dim_order\", con = engine, index = False)\n",
    "dim_date.to_sql(\"dim_date\", con = engine, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe how you'd schedule this process to run multiple times per day\n",
    "\n",
    "**Answer:** Using *CRONJOB* I can schedule the script to run multiple times per day where the data will be ingested from the API and loaded into the database, for example, lets assume that we want the process to run every 2 hours. using the **crontab configuration** shown below, we are able to achieve this.\n",
    "\n",
    "[![crontab.png](https://i.postimg.cc/V5P86MkG/crontab.png)](https://postimg.cc/JHxgYsJZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about if the data comes from a stream, and arrives at random times?\n",
    "\n",
    "**Answer:** In this case it makes more sense to use more advanced orchestration tools, Airflow is a good choice here, it's open source and very well developed. with Airflow we can create DAGS where we ensure that a specific transformation task doesn't run unless all the tasks that the transformation task depends on run first, with this we can ensure that even if data is coming in different times, a transformation is not triggered unless all the data have arrived successfully "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe how you would deploy your code to production and allow for future maitenance\n",
    "\n",
    "**Answer:** Deployment can take two paths, on-premise and in-cloud, let's assume that we want to deploy our code in cloud, AWS in this case. What we can do is first create an EC2 instance that will host our ETL script and a we can configure Airflow / CRONTAB to schedule our ETL. Also for maintenability, we will have to include the **logging** module in our ETL script and save our logs in a file, this will help us conduct root cause analysis on failures that have occured. Finally if there's any changes that we want to do to the ETL script, authorized users can SSH into the EC2 instance and apply those changes. \n",
    "\n",
    "So now we talked about the EC2 instance, but where will the data be loaded and how will it be accessible ?! For this, our ETL script will write all the output data (Star schema tables) into an S3 bucket (Data Lake), following that we will use **AWS Glue crawlers** to learn the schema's of the data stored in our data lake and save the schema's in an **AWS Glue Catalog**, now since we have all our schema's in **AWS Glue Catalog** we can create a redshift cluster and connect it to the AWS Glue Catalog using an external schema, by this our redshift cluster is able to query the data directly from the S3 Bucket (Data Lake), without the need to load the data in Redshift. This is cost effective since loading and keeping the data in S3 is way cost effective that loading and keeping the data in Redshift.\n",
    "\n",
    "For maintanenance, we have access to AWS cloudwatch logs to analyze any abnormalities / issues.\n",
    "\n",
    "Finally, here's the proposed architecture.\n",
    "\n",
    "[![Blank-diagram-2.jpg](https://i.postimg.cc/Jhw99hcr/Blank-diagram-2.jpg)](https://postimg.cc/VJRGtmVp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus point: What changes would you need to make to run this code in a containerized environment (e.g. Docker)?\n",
    "**Answer:** First of all, I will change my ETL code from a *notebook* style to a *standalone app* style with modules and packages, Second, I will create a package called **workflow** that will contain the following 3 modules: **Extractor.py**, **Transformer.py** and **Loader.py**, the extractor module will contain code that performs the data ingestion/extraction from the API, the **Transformer** module will contain code that's responsible for transforming the data (creating the schema tables) and the **Loader** module will contain code related to the loading of the data in the OLAP database. Then I will have a **main.py** script that will be the entry point to the ETL process. \n",
    "\n",
    "Once I have set up the app directory as stated above, I will create a **requirements.txt** file where I will include all the dependencies required to run the ETL script and finally I will create a dockerfile to create a docker image as shown below\n",
    "\n",
    "```\n",
    "FROM python:3\n",
    "\t\n",
    "COPY requirements.txt /tmp/\n",
    "\t\n",
    "RUN pip install --no-cache-dir --upgrade pip &&\\\n",
    "\t        pip install --requirement /tmp/requirements.txt\n",
    "\t\n",
    "COPY . /app\n",
    "\n",
    "CMD [ \"python\", \"/app/main.py\" ]\n",
    "```\n",
    "\n",
    "And then whenever we want to run the image, the etl script **(main.py)** will run automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Quiz time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Top 5 Nations By Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_nations_query = \"\"\"\n",
    "SELECT c.c_nation, SUM(l.l_revenue) AS \"revenue\"\n",
    "FROM dim_customer AS c JOIN lineitem_orders_fact AS l ON c.c_custkey = l.l_custkey\n",
    "GROUP BY c.c_nation\n",
    "ORDER BY SUM(l.l_revenue) DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "top_5_nations = sqlio.read_sql_query(top_5_nations_query, engine)\n",
    "top_5_nations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Shipmode Among Top 5 Nations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_shipmode_query = \"\"\"\n",
    "with cte as (\n",
    "    SELECT c.c_nation, l.l_shipmode, count(l.l_shipmode) AS \"shipmode_count\"\n",
    "    FROM dim_customer as c JOIN lineitem_orders_fact as l ON c.c_custkey = l.l_custkey\n",
    "    WHERE c.c_nation IN ('CANADA', 'EGYPT', 'IRAN', 'BRAZIL', 'ALGERIA')\n",
    "    GROUP BY c.c_nation, l.l_shipmode\n",
    "    ORDER BY c.c_nation DESC)\n",
    "\n",
    "SELECT l_shipmode, SUM(\"shipmode_count\") AS count\n",
    "FROM cte\n",
    "GROUP BY l_shipmode\n",
    "ORDER BY SUM(\"shipmode_count\") DESC\n",
    "LIMIT 1;\n",
    "\"\"\"\n",
    "\n",
    "most_common_shipmode = sqlio.read_sql_query(most_common_shipmode_query, engine)\n",
    "most_common_shipmode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Months By Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_selling_months_query = \"\"\"\n",
    "SELECT d.d_month, SUM(l.l_revenue) AS \"revenue\"\n",
    "FROM lineitem_orders_fact as l JOIN dim_date as d ON l.l_datekey = d.d_datekey\n",
    "GROUP BY d.d_month\n",
    "ORDER BY SUM(l.l_revenue) DESC\n",
    "LIMIT 3;\n",
    "\"\"\"\n",
    "\n",
    "top_3_months = sqlio.read_sql_query(top_3_selling_months_query, engine)\n",
    "top_3_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Customers By Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_customers_query = \"\"\"\n",
    "SELECT c.c_custkey, SUM(l.l_revenue) AS revenue\n",
    "FROM lineitem_orders_fact as l JOIN dim_customer as c ON l.l_custkey = c.c_custkey\n",
    "GROUP BY c.c_custkey\n",
    "ORDER BY SUM(l.l_revenue) DESC;\n",
    "\"\"\"\n",
    "\n",
    "top_customers = sqlio.read_sql_query(top_customers_query, engine)\n",
    "top_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financial Year to Year Revenue Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_yty_query = \"\"\"\n",
    "WITH cte AS (\n",
    "SELECT CASE WHEN \n",
    "\td.d_month::int BETWEEN 01 AND 06 THEN d.d_year::int ELSE d.d_year::int + 1 END as \"FinancialYear\", \n",
    "\tl.l_revenue AS \"revenue\"\n",
    "FROM dim_date as d JOIN lineitem_orders_fact as l on d.d_datekey = l.l_datekey\n",
    ")\n",
    "\n",
    "SELECT \n",
    "\t\"FinancialYear\", \n",
    "\tSUM(\"revenue\") AS \"Total_Year_Revenue\", \n",
    "\tLAG(SUM(\"revenue\")) OVER (ORDER BY \"FinancialYear\") AS \"Previous_Year_Revenue\",\n",
    "\t(SUM(\"revenue\") - LAG(SUM(\"revenue\")) OVER (ORDER BY \"FinancialYear\")) / SUM(\"revenue\") * 100 AS percent_change\n",
    "FROM cte\n",
    "GROUP BY \"FinancialYear\"\n",
    "ORDER BY \"FinancialYear\";\n",
    "\"\"\"\n",
    "\n",
    "financial_year_to_year = sqlio.read_sql_query(financial_yty_query, engine)\n",
    "financial_year_to_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api endpoint\n",
    "BASE_URL = \"https://cdn.moneyconvert.net/api/latest.json\"\n",
    "\n",
    "# api request\n",
    "MONEY_RATES = requests.get(BASE_URL).json()[\"rates\"]\n",
    "\n",
    "# aud_to_eur rate\n",
    "AUD_TO_EUR = MONEY_RATES[\"EUR\"] / MONEY_RATES[\"AUD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_eur_base(df, *columns):\n",
    "    \"\"\"transforms AUD based columns to EUR based\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to be transformed\n",
    "        *columns: list of columns to be transformed\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: pandas dataframe with transformed columns\n",
    "    \"\"\"    \n",
    "    table = df.copy()  # to avoid modifying the passed df since it's mutable\n",
    "\n",
    "    for col in columns:\n",
    "        table[col] = table[col] * AUD_TO_EUR\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_nations_EUR = transform_to_eur_base(top_5_nations, \"revenue\")\n",
    "top_5_nations_EUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_months_EUR = transform_to_eur_base(top_3_months, \"revenue\")\n",
    "top_3_months_EUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_customers_EUR = transform_to_eur_base(top_customers, \"revenue\")\n",
    "top_customers_EUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_yty_eur_based = transform_to_eur_base(financial_year_to_year, \"Total_Year_Revenue\", \"Previous_Year_Revenue\")\n",
    "financial_yty_eur_based"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e93bc1d23cbedd95de3332aab42618abd5298b480532deec74f2b89ef0ce0df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
